# ğŸ’¾ğŸ“€ Projeto de Engenharia de Dados

## ğŸ—„ğŸ‘¨â€ğŸ’¼ğŸ‘©â€ğŸ’¼ VisÃ£o Geral do Projeto

A empresa tem como objetivo entender o comportamento de compra dos clientes para oferecer um serviÃ§o personalizado.
Este projeto de engenharia de dados visa fornecer as bases necessÃ¡rias para a anÃ¡lise do comportamento de compra dos clientes.

## â™»ğŸ£ Requisitos de Dados

O pipeline de dados espera receber os seguintes campos de dados de vendas e clientes:

- Vendas: cÃ³digo, data, item, valor, quantidade, descontos, canais
- Clientes: cÃ³digo, data de nascimento, idade, gÃªnero, localizaÃ§Ã£o, flags de privacidade

## âš™ğŸ’¡ Requisitos do projeto

- [x] VersÃ£o 3.4.0 ou superior do Apache Spark
- [x] VersÃ£o 3.3.0 ou superior do Aoache Hadoop
- [x] VersÃ£o 3.6 atÃ© 3.10 do Python
- [x] Docker Engine ou Desktop
- [x] JDK11 ou JDK17 ou JDK21... ou outro Kit de Desenvolvimento Java equivalente ao Spark e Hadoop
- [x] Arquivos Jars: `delta-core_2.12-2.4.0.jar`, `delta-storage-2.4.0.jar` os dois sÃ£o obrigatÃ³rios.
- [x] [Consultar Release](https://docs.delta.io/latest/releases.html) de referÃªncia Delta Lake e Spark 

## ğŸğŸ  Pipeline de Dados

O processo de extraÃ§Ã£o, transformaÃ§Ã£o e carregamento dos dados necessÃ¡rios serÃ¡ realizado utilizando as seguintes etapas:

1. ExtraÃ§Ã£o: os dados foram entregues em um diretÃ³rio com arquivos `parquet` nomeados.
2. TransformaÃ§Ã£o: os dados serÃ£o limpos, validados e transformados em um formato adequado para anÃ¡lise.
3. Carregamento: os dados processados serÃ£o carregados em um repositÃ³rio ou banco de dados para acesso e anÃ¡lise.

## ğŸ”ğŸ” Garantia da Qualidade dos Dados

Para garantir a qualidade dos dados, serÃ£o utilizados mÃ©todos de validaÃ§Ã£o, limpeza e tratamentos necessÃ¡rios.
Isso incluirÃ¡ a verificaÃ§Ã£o de consistÃªncia, integridade e precisÃ£o dos dados.

## ğŸ“ğŸ“‚ [Proposta de Arquitetura](./docs/proposta.md)

A arquitetura proposta abordarÃ¡ a eficiÃªncia no processamento, escalabilidade e fluxo de dados entre diferentes camadas do pipeline.
SerÃ¡ adotada uma arquitetura modular e distribuÃ­da para lidar com grandes volumes de dados.

## ğŸ› âš’ Ferramentas e TÃ©cnicas

As ferramentas e tÃ©cnicas escolhidas serÃ£o baseadas nos requisitos do projeto, visando a eficiÃªncia e escalabilidade.
SerÃ£o utilizadas ferramentas de ETL (Extract, Transform, Load) para o processamento dos dados, bem como tecnologias de armazenamento e processamento distribuÃ­do.

## ğŸ‘¨ğŸ½â€ğŸ’¼â›‘ğŸ‘·ğŸ½â€â™‚ï¸ğŸ‘·ğŸ½â€â™€ï¸ Como executar o projeto?:
VocÃª pode copiar e  colar os comandos ou simplesmente executar em ordem.

* ExecuÃ§Ã£o local:
    ```Bash
    # Primeiro instalar as dependÃªncias Pipy em ambiente local:
    ## COM PIPENV ##
    python -m pip install pipenv # ApÃ³s instalar o pipenv se jÃ¡ nÃ£o tiver instalado localmente
    python -m pipenv install -r requirements.txt

    ## COM PIP PURO ##
    python -m pip upgrade pip
    python -m pip install -r requirements # A vantagem do pipenv Ã© que ele cria um ambiente dedicado ao projeto.

    # ApÃ³s instalar as dependÃªncias basta executar o script `main.py`
    python ./main.py
    ```
* ExecuÃ§Ã£o Docker
    ```bash
    # O Docker Ã© uma engine que cria e orquestra uma imagem de uma pseudo-mÃ¡quina virtual em linux 

    ## WSL ##
    docker build -t job_spark-i --build-arg BUILD_DATE=$(date -u +"%Y-%m-%dT%H:%M:%SZ") -f Dockerfile .

    ## PowerShell ##
    docker build -t job_spark-i --build-arg BUILD_DATE=$(Get-Date -Format "yyyy-MM-ddTHH:mm:ssZ") -f Dockerfile .

    #Docker Run - Cria e executa um container docker
    docker run -it  --name job_spark-c job_spark-i:latest
    #Docker Run - Cria e executa um container docker mapeando a porta
    docker run -p 4040:4040 -dit  --name job_spark-c job_spark-i:latest
    #Docker Run - Cria e executa um container docker mapeando a porta e o volume de execuÃ§Ã£o
    docker run -p 4040:4040 -dit --name job_spark-c -v .:./app job_spark-i:latest
    ```
* Executando na nuvem AWS - exemplo
    ```bash
    ## WSL ##
    docker build -t job_spark-i --build-arg BUILD_DATE=$(date -u +"%Y-%m-%dT%H:%M:%SZ") -f Dockerfile .

    ## PowerShell ##
    docker build -t job_spark-i --build-arg BUILD_DATE=$(Get-Date -Format "yyyy-MM-ddTHH:mm:ssZ") -f Dockerfile .

    # Para fazer o upload da imagem para o ECR, siga estes passos:

    # Crie um repositÃ³rio ECR na AWS Console ou usando o AWS CLI.
    # FaÃ§a login no ECR usando o comando: 
    aws ecr get-login-password --region sa-east-1 | docker login --username AWS --password-stdin id_da_conta.dkr.ecr.regiao.amazonaws.com. 

    # Substitua  "id_da_conta" pelo ID da sua conta AWS.
    # Marque a imagem que vocÃª construiu com o comando: 
    docker tag job_spark-i:`tag` `id_da_conta`.dkr.ecr.regiao.amazonaws.com/`nome_do_repositorio`:`tag`. 

    # Substitua "tag" pela tag da imagem, 
    # "id_da_conta.dkr.ecr.regiao.amazonaws.com" pelo endpoint do seu repositÃ³rio ECR e "`nome_do_repositorio`:`tag`" pelo nome do repositÃ³rio ECR e a tag desejada.
    # FaÃ§a o push da imagem para o ECR com o comando: 
    docker push `id_da_conta`.dkr.ecr.sa-east-1.amazonaws.com/`nome_do_repositorio`:`tag`.

    # Configure o ECS para consumir a imagem do ECR e depois crie uma DefiniÃ§Ã£o de TareÃ§a vingulada ao container e a imagem
    # ApÃ³s isso basta definir um CRON e aguardar a execuÃ§Ã£o
    ```

## â„¹ ConsideraÃ§Ãµes Adicionais

Para projetos futuros, serÃ¡ importante considerar a implementaÃ§Ã£o de tÃ©cnicas avanÃ§adas de anÃ¡lise de dados, como machine learning,
para oferecer insights mais precisos sobre o comportamento de compra dos clientes.

## ğŸ§‘ğŸ½ Colaboradores

Este projeto foi criado por:

- Gustavo H Martins ([GitHub](https://github.com/Gustavo-H-Martins) | [LinkedIn](https://www.linkedin.com/in/gustavo-henrique-lopes-martins-361789192/))
