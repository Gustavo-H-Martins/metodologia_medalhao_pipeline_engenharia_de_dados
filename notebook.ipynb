{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jobs import Sparkinit\n",
    "from json import dump, dumps\n",
    "from utils import formatar_sql\n",
    "spark_start = Sparkinit()\n",
    "import os\n",
    "spark = spark_start.buscar_sessao_spark()\n",
    "\n",
    "print(f\"WebUI SparkJobs: {spark.sparkContext.uiWebUrl}\")\n",
    "spark.getActiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Carregando o dataframe para verificar a estrutura das colunas e tipos de dados\n",
    "path = os.path.abspath(r\"C:\\Users\\gustavo.lopes\\Documentos\\GitHub\\metodologia_medalhao_pipeline_engenharia_de_dados\\datalake\\camada_0_transient\\VENDAS\")\n",
    "dados = spark.read.format(\"parquet\").load(path)\n",
    "\n",
    "dados.printSchema()\n",
    "\n",
    "dados.show(n=1, vertical=True)\n",
    "colunas = ',\\n\\t'.join(dados.columns)\n",
    "\n",
    "base = [f'CAST({col} AS STRING)' for col in dados.columns]\n",
    "base_media = ' , '.join(base)\n",
    "base_final = f\"BASE64(CONCAT({base_media})) AS hash_id\"\n",
    "base_final\n",
    "\n",
    "f\"\"\"/* 'Bronze' */\n",
    "SELECT \n",
    "        {colunas},\n",
    "        {base_final}\n",
    "FROM vendas_tmp\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a tabela temporária e realizando a consulta que depois usaremos para transformar a camada raw\n",
    "dados.createOrReplaceTempView(\"vendas_tmp\")\n",
    "\n",
    "query = formatar_sql(\"\"\"/* 'Silver' */\n",
    "SELECT \n",
    "    COALESCE(date_format(vt.d_dt_vd, 'yyyy-MM-dd HH:mm:ss'), '') AS data_emissao,\n",
    "    CAST(vt.n_id_fil AS BIGINT) AS codigo_filial,\n",
    "    CAST(vt.n_id_vd_fil AS BIGINT) AS id_venda_filial,\n",
    "    COALESCE(CAST(vt.v_cli_cod AS STRING), '') AS codigo_cliente,\n",
    "    CAST(vt.n_vlr_tot_vd AS DECIMAL(38, 2)) AS valor_total_venda,\n",
    "    CAST(vt.n_vlr_tot_desc AS DECIMAL(38, 2)) AS valor_total_desconto,\n",
    "    CASE \n",
    "        WHEN vt.v_cpn_eml  = 'SIM' THEN True\n",
    "        ELSE False\n",
    "    END AS enviado_email,\n",
    "    COALESCE(CAST(vt.tp_pgt AS STRING), '') AS tipo_pagamento\n",
    "    \n",
    "FROM vendas_tmp as vt\"\"\")\n",
    "\n",
    "spark.sql(query).show()\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pedidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carregando o dataframe para verificar a estrutura das colunas e tipos de dados\n",
    "dados = spark.read.format(\"parquet\").load(os.path.abspath(r\"C:\\Users\\gustavo.lopes\\Documentos\\GitHub\\metodologia_medalhao_pipeline_engenharia_de_dados\\datalake\\camada_0_transient\\PEDIDOS\"))\n",
    "\n",
    "dados.printSchema()\n",
    "\n",
    "dados.show(n=1, vertical=True)\n",
    "colunas = ',\\n\\t'.join(dados.columns)\n",
    "base = [f'CAST({col} AS STRING)' for col in dados.columns]\n",
    "base_media = ' , '.join(base)\n",
    "base_final = f\"BASE64(CONCAT({base_media})) AS hash_id\"\n",
    "base_final\n",
    "\n",
    "f\"\"\"/* 'Bronze' */\n",
    "SELECT \n",
    "        {colunas}, \n",
    "        {base_final}\n",
    "FROM pedidos_tmp\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a tabela temporária e realizando a consulta que depois usaremos para transformar a camada raw\n",
    "dados.createOrReplaceTempView(\"pedidos_tmp\")\n",
    "\n",
    "query = formatar_sql(\"\"\"/* 'Silver' */\n",
    "SELECT \n",
    "    CAST(pd.n_id_pdd AS BIGINT) AS id_pedido,\n",
    "    COALESCE(CAST(pd.d_dt_eft_pdd AS DATE), '') AS data_realizacao_pedido,\n",
    "    COALESCE(date_format(pd.d_dt_entr_pdd, 'yyyy-MM-dd HH:mm:ss'), '') AS data_entrega,\n",
    "    CASE \n",
    "        WHEN pd.v_cnl_orig_pdd = 'L' THEN 'Loja'\n",
    "        WHEN pd.v_cnl_orig_pdd = 'A' THEN 'App'\n",
    "        WHEN pd.v_cnl_orig_pdd = 'S' THEN 'Site'\n",
    "    END AS canal_venda,\n",
    "    COALESCE(CAST(pd.v_uf_entr_pdd AS STRING), '') AS UF_pedido,\n",
    "    COALESCE(CAST(pd.v_lc_ent_pdd AS STRING), '') AS cidade_entrega,\n",
    "    CAST(pd.n_vlr_tot_pdd AS DECIMAL(38,2)) AS valor_total_pedido\n",
    "    BASE64(CAST(n_id_pdd AS STRING)) AS hash_id\n",
    "FROM pedidos_tmp as pd\"\"\")\n",
    "\n",
    "spark.sql(query).show()\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pedidos Vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carregando o dataframe para verificar a estrutura das colunas e tipos de dados\n",
    "dados = spark.read.format(\"parquet\").load(os.path.abspath(r\"C:\\Users\\gustavo.lopes\\Documentos\\GitHub\\metodologia_medalhao_pipeline_engenharia_de_dados\\datalake\\camada_0_transient\\PEDIDO_VENDA\"))\n",
    "\n",
    "dados.printSchema()\n",
    "\n",
    "dados.show(n=1, vertical=True)\n",
    "colunas = ',\\n\\t'.join(dados.columns)\n",
    "base = [f'CAST({col} AS STRING)' for col in dados.columns]\n",
    "base_media = ' , '.join(base)\n",
    "base_final = f\"BASE64(CONCAT({base_media})) AS hash_id\"\n",
    "base_final\n",
    "f\"\"\"/* 'Bronze' */\n",
    "SELECT \n",
    "        {colunas},\n",
    "        {base_final} \n",
    "FROM pedido_venda_tmp\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a tabela temporária e realizando a consulta que depois usaremos para transformar a camada raw\n",
    "dados.createOrReplaceTempView(\"pedido_venda_tmp\")\n",
    "\n",
    "query = formatar_sql(\"\"\"/* 'Silver'*/\n",
    "SELECT \n",
    "    CAST(pv.n_id_fil AS BIGINT) AS codigo_filial,\n",
    "    CAST(pv.n_id_vd_fil AS BIGINT) AS id_venda_filial,\n",
    "    CAST(pv.n_id_pdd AS BIGINT) AS id_pedido\n",
    "    BASE64(CONCAT(CAST(n_id_fil AS STRIN), CAST(n_id_vd_fil AS STRING), CAST(n_id_pdd AS STRING))) AS hash_id\n",
    "FROM pedido_venda_tmp as pv\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(query).show()\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Itens Vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carregando o dataframe para verificar a estrutura das colunas e tipos de dados\n",
    "dados = spark.read.format(\"parquet\").load(os.path.abspath(r\"C:\\Users\\gustavo.lopes\\Documentos\\GitHub\\metodologia_medalhao_pipeline_engenharia_de_dados\\datalake\\camada_0_transient\\ITENS_VENDAS\"))\n",
    "\n",
    "dados.printSchema()\n",
    "\n",
    "dados.show(n=1, vertical=True)\n",
    "colunas = ',\\n\\t'.join(dados.columns)\n",
    "base = [f'CAST({col} AS STRING)' for col in dados.columns]\n",
    "base_media = ' , '.join(base)\n",
    "base_final = f\"BASE64(CONCAT({base_media})) AS hash_id\"\n",
    "base_final\n",
    "f\"\"\"/* 'Bronze' */\n",
    "SELECT \n",
    "        {colunas},\n",
    "        {base_final}\n",
    "FROM itens_vendas_tmp\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a tabela temporária e realizando a consulta que depois usaremos para transformar a camada raw\n",
    "dados.createOrReplaceTempView(\"itens_vendas_tmp\")\n",
    "\n",
    "query = formatar_sql(\"\"\"/* 'Silver' */\n",
    "SELECT \n",
    "    CAST(iv.n_id_fil AS BIGINT) AS codigo_filial,\n",
    "    CAST(iv.n_id_vd_fil AS BIGINT) AS id_venda_filial,\n",
    "    CAST(iv.n_id_it AS BIGINT) AS codigo_item_venda,\n",
    "    CASE \n",
    "        WHEN iv.v_rc_elt  = 'SIM' THEN True\n",
    "        ELSE False\n",
    "    END AS com_receita_eletronica,\n",
    "    CASE \n",
    "        WHEN iv.v_it_vd_conv = 'SIM' THEN 'Convênio'\n",
    "        WHEN iv.v_it_vd_conv = 'NAO' AND iv.n_vlr_desc > 0 THEN 'Promoção'\n",
    "        ELSE 'Sem Desconto'\n",
    "    END AS tipo_desconto,\n",
    "    CAST(iv.n_vlr_pis AS DECIMAL(38,2)) AS valor_pis_item,\n",
    "    CAST(iv.n_vlr_vd AS DECIMAL(38,2)) AS valor_final_item,\n",
    "    CAST(iv.n_vlr_desc AS DECIMAL(38,2)) AS valor_desconto_item,\n",
    "    CAST(iv.n_qtd AS BIGINT) AS quantidade_itens,\n",
    "FROM itens_vendas_tmp as iv\"\"\")\n",
    "\n",
    "spark.sql(query).show()\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endereços Clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carregando o dataframe para verificar a estrutura das colunas e tipos de dados\n",
    "dados = spark.read.format(\"parquet\").load(os.path.abspath(r\"C:\\Users\\gustavo.lopes\\Documentos\\GitHub\\metodologia_medalhao_pipeline_engenharia_de_dados\\datalake\\camada_0_transient\\ENDERECOS_CLIENTES\"))\n",
    "\n",
    "dados.printSchema()\n",
    "\n",
    "dados.show(n=1, vertical=True)\n",
    "colunas = ',\\n\\t'.join(dados.columns)\n",
    "base = [f'CAST({col} AS STRING)' for col in dados.columns]\n",
    "base_media = ' , '.join(base)\n",
    "base_final = f\"BASE64(CONCAT({base_media})) AS hash_id\"\n",
    "base_final\n",
    "f\"\"\"/* 'Bronze' */\n",
    "SELECT \n",
    "        {colunas},\n",
    "        {base_final}\n",
    "FROM enderecos_clientes_tmp\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a tabela temporária e realizando a consulta que depois usaremos para transformar a camada raw\n",
    "dados.createOrReplaceTempView(\"enderecos_clientes_tmp\")\n",
    "\n",
    "query = formatar_sql(\"\"\"/* 'Silver' */\n",
    "SELECT \n",
    "    CAST(ec.v_id_cli AS STRING) AS codigo_cliente,\n",
    "    CAST(ec.n_sq_end AS BIGINT) AS sequencia_endereco_cliente,\n",
    "    COALESCE(date_format(ec.d_dt_exc, 'yyyy-MM-dd HH:mm:ss'), '') AS data_exclusao_endereco,\n",
    "    CAST(ec.v_lcl AS STRING) AS cidade_endereco,\n",
    "    CAST(ec.v_uf AS STRING) AS UF_endereco\n",
    "FROM enderecos_clientes_tmp as ec\"\"\")\n",
    "\n",
    "spark.sql(query).show()\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clientes Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o dataframe para verificar a estrutura das colunas e tipos de dados\n",
    "dados = spark.read.option(\"multiline\",\"true\").format(\"json\").load(os.path.abspath(r\"C:\\Users\\gustavo.lopes\\Documentos\\GitHub\\metodologia_medalhao_pipeline_engenharia_de_dados\\datalake\\camada_0_transient\\CLIENTES_OPT\"))\n",
    "\n",
    "dados.printSchema()\n",
    "\n",
    "dados.show(n=2, vertical=True)\n",
    "colunas = ',\\n\\t'.join(dados.columns)\n",
    "base = ['CAST({} AS STRING)'.format(col) for col in dados.columns]\n",
    "base_media = ' , '.join(base)\n",
    "base_final = f\"BASE64(CONCAT({base_media})) AS hash_id\"\n",
    "base_final\n",
    "f\"\"\"/* 'Bronze' */\n",
    "SELECT \n",
    "        {colunas},\n",
    "        {base_final}\n",
    "FROM clientes_opt_tmp\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a tabela temporária e realizando a consulta que depois usaremos para transformar a camada raw\n",
    "dados.createOrReplaceTempView(\"clientes_opt_tmp\")\n",
    "\n",
    "query = formatar_sql(\"\"\"/* 'Silver' */\n",
    "SELECT \n",
    "    CAST(co.v_id_cli AS STRING) AS codigo_cliente,\n",
    "    CASE \n",
    "        WHEN co.b_push  = True THEN 'SIM'\n",
    "        WHEN co.b_push  = False THEN 'NÃO'\n",
    "        ELSE ''\n",
    "    END AS autoriza_notificacao_push,\n",
    "    CASE \n",
    "        WHEN co.b_sms  = True THEN 'SIM'\n",
    "        WHEN co.b_sms  = False THEN 'NÃO'\n",
    "        ELSE ''\n",
    "    END AS autoriza_notificacao_sms,\n",
    "    CASE \n",
    "        WHEN co.b_email  = True THEN 'SIM'\n",
    "        WHEN co.b_email  = False THEN 'NÃO'\n",
    "        ELSE ''\n",
    "    END AS autoriza_notificacao_email,\n",
    "    CASE \n",
    "        WHEN co.b_call  = True THEN 'SIM'\n",
    "        WHEN co.b_call  = False THEN 'NÃO'\n",
    "        ELSE ''\n",
    "    END AS autoriza_notificacao_ligacao\n",
    "FROM clientes_opt_tmp as co\"\"\")\n",
    "\n",
    "spark.sql(query).show()\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o dataframe para verificar a estrutura das colunas e tipos de dados\n",
    "dados = spark.read.format(\"parquet\").load(os.path.abspath(os.path.join(os.getcwd(), \"datalake/camada_0_transient/CLIENTES\")))\n",
    "\n",
    "dados.printSchema()\n",
    "\n",
    "dados.show(n=1, vertical=True)\n",
    "colunas = ',\\n\\t'.join(dados.columns)\n",
    "base = ['CAST({} AS STRING)'.format(col) for col in dados.columns]\n",
    "base_media = ' , '.join(base)\n",
    "base_final = f\"BASE64(CONCAT({base_media})) AS hash_id\"\n",
    "base_final\n",
    "print(dumps(f\"\"\"/* 'Bronze' */\n",
    "SELECT \n",
    "        {colunas},\n",
    "        {base_final}\n",
    "FROM clientes_tmp\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a tabela temporária e realizando a consulta que depois usaremos para transformar a camada raw\n",
    "dados.createOrReplaceTempView(\"clientes_tmp\")\n",
    "\n",
    "query = formatar_sql(\"\"\"/* 'Silver' */\n",
    "SELECT \n",
    "    CAST(c.v_id_cli AS STRING) AS codigo_cliente,\n",
    "    COALESCE(CAST(c.d_dt_nasc AS DATE), '') AS data_nascimento_cliente,\n",
    "    CAST(DATEDIFF(CURRENT_DATE(), CAST(c.d_dt_nasc AS DATE)) / 365 AS INT) as idade,\n",
    "    CASE \n",
    "        WHEN c.v_sx_cli  = 'F' THEN 'Feminino'\n",
    "        WHEN c.v_sx_cli  = 'M' THEN 'Masculino'\n",
    "        ELSE '' \n",
    "    END AS genero_biologico_cliente,\n",
    "    CASE \n",
    "        WHEN c.n_est_cvl  = 1 THEN 'Solteiro'\n",
    "        WHEN c.n_est_cvl  = 2 THEN 'Casado'\n",
    "        WHEN c.n_est_cvl  = 3 THEN 'Viúvo'\n",
    "        WHEN c.n_est_cvl  = 4 THEN 'Desquitado'\n",
    "        WHEN c.n_est_cvl  = 5 THEN 'Divorciado'\n",
    "        WHEN c.n_est_cvl  = 6 THEN 'Outros'\n",
    "        ELSE ''\n",
    "    END AS estado_civil_cliente\n",
    "FROM clientes_tmp as c\"\"\")\n",
    "spark.sql(query).show()\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" -- 'Tabela Endereços Clientes parquet'\n",
    "SELECT \n",
    "    CAST(c.v_id_cli AS STRING) AS codigo_cliente,\n",
    "    COALESCE(CAST(c.d_dt_nasc AS DATE), '') AS data_nascimento_cliente,\n",
    "    CASE \n",
    "        WHEN c.v_sx_cli  = 'F' THEN 'Feminino'\n",
    "        WHEN c.v_sx_cli  = 'M' THEN 'Masculino'\n",
    "        ELSE '' \n",
    "    END AS genero_biologico_cliente,\n",
    "    CASE \n",
    "        WHEN c.n_est_cvl  = 1 THEN 'Solteiro'\n",
    "        WHEN c.n_est_cvl  = 2 THEN 'Casado'\n",
    "        WHEN c.n_est_cvl  = 3 THEN 'Viúvo'\n",
    "        WHEN c.n_est_cvl  = 4 THEN 'Desquitado'\n",
    "        WHEN c.n_est_cvl  = 5 THEN 'Divorciado'\n",
    "        WHEN c.n_est_cvl  = 6 THEN 'Outros'\n",
    "        ELSE ''\n",
    "    END AS estado_civil_cliente\n",
    "FROM clientes_tmp as c\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" /* Gold 'Vendas' */\n",
    "SELECT \n",
    "    v.codigo_filial AS codifo_filial,\n",
    "    v.id_venda_filial AS codigo_cupom_venda,\n",
    "    v.data_emissao AS data_emissao,\n",
    "    iv.codigo_item_venda AS codigo_item,\n",
    "    iv.valor_final_item AS valor_unitario,\n",
    "    iv.quantidade_itens AS quantidade,\n",
    "    v.codigo_cliente AS codigo_cliente,\n",
    "    iv.tipo_desconto AS tipo_desconto,\n",
    "    p.canal_venda AS canal_venda\n",
    "FROM vendas AS v\n",
    "INNER JOIN pedido_venda AS pv ON CONCAT(CAST(pv.codigo_filial AS STRING), CAST(pv.id_venda_filial AS STRING)) = CONCAT(CAST(v.codigo_filial AS STRING), CAST(v.id_venda_filial AS STRING))\n",
    "INNER JOIN itens_vendas AS iv ON CONCAT(CAST(iv.codigo_filial AS STRING), CAST(iv.id_venda_filial)) = CONCAT(CAST(v.codigo_filial AS STRING), CAST(v.id_venda_filial AS STRING))\n",
    "INNER JOIN pedidos AS p ON p.id_pedido = pv.id_pedido\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"/* 'Silver' */\\nSELECT COALESCE(date_format(vt.d_dt_vd, 'yyyy-MM-dd HH:mm:ss'), '') AS data_emissao,\\n       CAST(vt.n_id_fil AS BIGINT) AS codigo_filial,\\n       CAST(vt.n_id_vd_fil AS BIGINT) AS id_venda_filial,\\n       COALESCE(CAST(vt.v_cli_cod AS STRING), '') AS codigo_cliente,\\n       CAST(vt.n_vlr_tot_vd AS DECIMAL(38, 2)) AS valor_total_venda,\\n       CAST(vt.n_vlr_tot_desc AS DECIMAL(38, 2)) AS valor_total_desconto,\\n       CASE\\n           WHEN vt.v_cpn_eml = 'SIM' THEN TRUE\\n           ELSE FALSE\\n       END AS enviado_email,\\n       COALESCE(CAST(vt.tp_pgt AS STRING), '') AS tipo_pagamento, \\nhash_id\\nFROM vendas_tmp AS vt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"/* Gold 'Clientes' */\n",
    "SELECT\n",
    "    c.codigo_cliente AS codigo_cliente,\n",
    "    c.data_nascimento_cliente AS data_nascimento,\n",
    "    DATEDIFF('YEAR', CAST(data_nascimento AS DATE), CURRENT_DATE()) as idade,\n",
    "    c.genero_biologico_cliente AS sexo,\n",
    "    ec.UF_endereco AS uf,\n",
    "    ec.cidade_endereco AS cidade,\n",
    "    c.estado_civil_cliente AS estado_civil,\n",
    "    co.autoriza_notificacao_ligacao AS flag_lgpd_call,\n",
    "    co.autoriza_notificacao_sms AS flag_lgpd_sms,\n",
    "    co.autoriza_notificacao_email AS flag_lgpd_email,\n",
    "    co.autoriza_notificacao_push AS flag_lgpd_push\n",
    "FROM clientes AS c\n",
    "INNER JOIN clientes_opt AS co ON co.codigo_cliente = c.codigo_cliente\n",
    "INNER JOIN enderecos_clientes AS ec ON ec.codigo_cliente = c.codigo_cliente\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desafio_panvel-data_engineer-pE6NpJI7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
